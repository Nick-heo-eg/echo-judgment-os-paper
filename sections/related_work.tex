\section{Background and Related Work}
\subsection{Model-Level Alignment}
RLHF, preference modeling, and constitutional AI emphasize reward shaping but remain vulnerable to specification gaming~\cite{anthropic2023deception}. They reinforce behaviors without interrogating ontology, so deception can remain latent.

\subsection{External Oversight Models}
Rule-based overseers (e.g., guardrails, policy filters) introduce second-order models that themselves become black boxes. Prior work such as Gemini Safety Cards acknowledges oversight limitations under test-aware adversaries~\cite{gemini2024safety}.

\subsection{Interpretability and Deception}
Interpretability research~\cite{apollo2024compliance} documents ``model situational awareness'' that defeats standard heuristics. Without an external identity anchor, detecting deception collapses into interpreting representations that already wish to mislead.

\subsection{Deterministic Layers}
Symbolic policy engines and neuro-symbolic hybrids offer deterministic control, yet typically reinsert themselves into the generation loop, thus reintroducing latency and enforcement semantics. Echo OS diverges by centering identity as the runtime itself and relegating models to replaceable nodes.
